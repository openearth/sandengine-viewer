{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aeolian transport data\n",
    "1) Get data from opendap\n",
    "2) Output GeoJson with locations\n",
    "3) Output json per location with timeseries of particle count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import netCDF4 as nc\n",
    "import geojson\n",
    "import json\n",
    "import urllib\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Get dataset from opendap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deployment_20141021224500_20141023000000_045_46318725_dn11.nc', 'deployment_20141021134000_20141021224500_045_46318725_dn10.nc', 'deployment_20141020051500_20141021134000_090_46318725_dn9.nc', 'deployment_20141013130000_20141020051500_135_46318725_dn8.nc', 'deployment_20141006103000_20141013130000_135_8157_dn6.nc', 'deployment_20141006103000_20141013130000_135_46382_dn7.nc', 'deployment_20141006103000_20141013130000_135_46382157_dn67.nc', 'deployment_20140923100000_20141006103000_090_43825761_dn5.nc', 'deployment_20140921160000_20140923100000_000_374_dn4.nc', 'deployment_20140920180000_20140921160000_098_3748_dn2.nc', 'deployment_20140920180000_20140921160000_000_6125_dn3.nc', 'deployment_20140917000000_20140920180000_098_23748561_dn1.nc']\n"
     ]
    }
   ],
   "source": [
    "url_cat = r'http://opendap.tudelft.nl/thredds/catalog/data2/zandmotor/aeolian/megapex/catalog.html'\n",
    "url_cat_source = str(urllib.request.urlopen(url_cat).read())\n",
    "\n",
    "# extract filenames attempt 1\n",
    "filenames = []\n",
    "url_cat_source_marker = \"<a href=\\'catalog.html?dataset=[0-9,a-z,A-Z,_,/,.]*.nc\\'><tt>\"\n",
    "for line in re.findall(url_cat_source_marker, url_cat_source):\n",
    "    filenames.append(line[:-6].split(url_cat)[1])\n",
    "    \n",
    "# extract filenames attempt 2\n",
    "filenames = []\n",
    "for i in range(len(url_cat_source.split('<tt>'))):\n",
    "    if ('.nc' in url_cat_source.split('<tt>')[i]) & ('deployment' in url_cat_source.split('<tt>')[i]):\n",
    "        filenames.append(url_cat_source.split('<tt>')[i].split('<a')[-1].split('/')[-1].replace(\"'>\",''))\n",
    "\n",
    "# filter on merged Netcdf\n",
    "filenames2 = []\n",
    "for i in range(len(filenames)):\n",
    "    if ('dn' in filenames[i]) & ('_merged' not in filenames[i]):\n",
    "        filenames2.append(filenames[i])\n",
    "        \n",
    "print(filenames2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Store all locations in GeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment 0\n",
      "deployment 1\n",
      "deployment 2\n",
      "deployment 3\n",
      "deployment 4\n",
      "deployment 5\n",
      "deployment 6\n",
      "deployment 7\n",
      "deployment 8\n",
      "deployment 9\n",
      "deployment 10\n",
      "deployment 11\n"
     ]
    }
   ],
   "source": [
    "# put to geojson\n",
    "features = []\n",
    "\n",
    "# loop over deployments\n",
    "for k in range(len(filenames2)):\n",
    "    print('deployment',k)\n",
    "    url = r'http://opendap.tudelft.nl/thredds/dodsC/data2/zandmotor/aeolian/megapex/' + filenames2[k]\n",
    "    dataset = nc.Dataset(url)\n",
    "    dataset.set_auto_mask(False)\n",
    "    \n",
    "    # get general deployment information\n",
    "    deploymentName = filenames2[k].split('_')[-1].replace('.nc','')\n",
    "    \n",
    "    tstart = datetime.datetime.strptime(filenames2[k].split('_')[1],'%Y%m%d%H%M%S')\n",
    "    tend = datetime.datetime.strptime(filenames2[k].split('_')[2],'%Y%m%d%H%M%S')\n",
    "\n",
    "    # put locations to json\n",
    "    for i in range(len(dataset['lat'])):\n",
    "        p = geojson.Point((float(dataset['lon'][i]),float(dataset['lat'][i])))\n",
    "        feature = geojson.Feature(geometry=p,properties={'deploymentName': deploymentName,\n",
    "                                                         'timeStart':str(int(tstart.timestamp())),\n",
    "                                                         'timeEnd':str(int(tend.timestamp())),\n",
    "                                                         'location_ID': '%s_%i' %('location_ID', i)})\n",
    "        features.append(feature)\n",
    "\n",
    "featureCollection = geojson.FeatureCollection(features)\n",
    "\n",
    "f = open(r'../static/AeolianLocation.geojson','w')\n",
    "f.write(json.dumps(featureCollection))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Store timeseries in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dn11\n",
      "dn10\n",
      "dn9\n",
      "dn8\n",
      "dn6\n",
      "dn7\n",
      "dn67\n",
      "dn5\n",
      "dn4\n",
      "dn2\n",
      "dn3\n",
      "dn1\n"
     ]
    }
   ],
   "source": [
    "# loop over deployments, output single json per deployment\n",
    "for i in range(len(filenames2)):\n",
    "    deployments = {}\n",
    "\n",
    "    url = r'http://opendap.tudelft.nl/thredds/dodsC/data2/zandmotor/aeolian/megapex/' + filenames2[i]\n",
    "    dataset = nc.Dataset(url)\n",
    "    dataset.set_auto_mask(False)\n",
    "    \n",
    "    # get general deployment information\n",
    "    deploymentName = filenames2[i].split('_')[-1].replace('.nc','')\n",
    "    print(deploymentName)\n",
    "    \n",
    "    tstart = datetime.datetime.strptime(filenames2[i].split('_')[1],'%Y%m%d%H%M%S')\n",
    "    tend = datetime.datetime.strptime(filenames2[i].split('_')[2],'%Y%m%d%H%M%S')\n",
    "    \n",
    "    time = dataset['time'][:]\n",
    "    \n",
    "    # loop over locations\n",
    "    locations = {}\n",
    "    locations['time'] = time.tolist()\n",
    "    # loop over locations within deployment\n",
    "    for j in range(np.shape(dataset['particle_count_1'])[1]):        \n",
    "        # loop over height\n",
    "        \n",
    "        heights = {}\n",
    "        for k in range(10):\n",
    "            name = 'particle_count_' + str(k)\n",
    "            if  name in dataset.variables:\n",
    "                particle_count = dataset['particle_count_' + str(k)][:,j]\n",
    "                particle_count[particle_count < 0] = 0\n",
    "                particle_count = particle_count.tolist()\n",
    "                particle_height = dataset['particle_count_' + str(k)].height\n",
    "\n",
    "                heightproperties = {}\n",
    "                heightproperties['height'] = particle_height\n",
    "                heightproperties['particle_counts'] = particle_count\n",
    "                \n",
    "                heights[name] = heightproperties\n",
    "                \n",
    "        locations['%s_%i' %('location_ID', j)] = heights\n",
    "    deployments[deploymentName] = locations\n",
    "    \n",
    "    # store\n",
    "    f = open('%s_%s%s' %(r'../static/aeolian_data', deploymentName, '.json'),'w')\n",
    "    f.write(json.dumps(deployments))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "particle_count_4 not found in /",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-172-f9da323bb671>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mparticle_count_3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparticle_count_3\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mparticle_count_4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'particle_count_4'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mparticle_count_4\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparticle_count_4\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnetCDF4\\_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__getitem__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: particle_count_4 not found in /"
     ]
    }
   ],
   "source": [
    "# define location\n",
    "deploymentName = 'dn1'\n",
    "locationID ='loc_0'\n",
    "\n",
    "# get timeseries of particle count at those locations\n",
    "particle_count_1 = np.sum(dataset['particle_count_1'][:],1)\n",
    "particle_count_1[particle_count_1 < 0] = 0\n",
    "\n",
    "particle_count_2 = np.sum(dataset['particle_count_2'][:],1)\n",
    "particle_count_2[particle_count_2 < 0] = 0\n",
    "\n",
    "particle_count_3 = np.sum(dataset['particle_count_3'][:],1)\n",
    "particle_count_3[particle_count_3 < 0] = 0\n",
    "\n",
    "particle_count_4 = np.sum(dataset['particle_count_4'][:],1)\n",
    "particle_count_4[particle_count_4 < 0] = 0\n",
    "\n",
    "heightvalues = [dataset['particle_count_1'].height, dataset['particle_count_2'].height, dataset['particle_count_3'].height, dataset['particle_count_4'].height] # [m]\n",
    "print(heightvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03\n",
      "0.1\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "\n",
    "for height_ID in deployments[deploymentName][locationID].keys():\n",
    "    if height_ID != 'locationID':\n",
    "        particle_count = np.array(deployments[deploymentName][locationID][height_ID]['particle_counts'])\n",
    "        particle_count[particle_count < 0] = 0\n",
    "        \n",
    "        height = deployments[deploymentName][locationID][height_ID]['height']\n",
    "        \n",
    "        plt.plot(time, particle_count)\n",
    "plt.ylabel('Particles')\n",
    "plt.xlim(time[0],time[-1])\n",
    "plt.ylim(0,np.max(particle_count_1))\n",
    "plt.title('%s %.2f %s' %('Particle count at height', heightvalues[0], 'm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time = np.array(deployments[deploymentName]['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "descriptor 'timestamp' requires a 'datetime.datetime' object but received a 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-203-7acc748e4c31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: descriptor 'timestamp' requires a 'datetime.datetime' object but received a 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "datetime.datetime.timestamp(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
